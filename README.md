# üß† Experimento Cartesiano Computacional  
**An√°lise filos√≥fica atrav√©s de LLMs com m√©todo cartesiano**  

![License](https://img.shields.io/badge/License-MIT-green) 
![Python](https://img.shields.io/badge/Python-3.8%2B-blue) 
![LLM](https://img.shields.io/badge/LLM-llama.cpp-yellow)

Este projeto implementa uma an√°lise filos√≥fica seguindo o m√©todo cartesiano, utilizando modelos de linguagem (LLM) para processar quest√µes complexas. O objetivo √© decompor uma pergunta filos√≥fica em subquest√µes elementares, gerar an√°lises iniciais e refin√°-las em v√°rios ciclos para obter uma compreens√£o mais profunda do problema.

## Estrutura do Projeto

O projeto √© composto pelos seguintes arquivos e diret√≥rios:

- **`config_default.json`**: Arquivo de configura√ß√£o que define par√¢metros como o caminho do modelo, n√∫mero de threads, temperatura, etc.
- **`decartes.py`**: Script principal que implementa a classe `ExperimentoCartesiano` e realiza a an√°lise.
- **`resultados/`**: Diret√≥rio onde os resultados das an√°lises s√£o salvos, incluindo logs, an√°lises iniciais e refinamentos.
- **LLM**: DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf vers√£o quantizada

## Como Usar

### 1. Instala√ß√£o

Para executar o Experimento Cartesiano, voc√™ precisar√° configurar o ambiente corretamente. Siga os passos abaixo:

#### 1.1. Verifique a vers√£o do Python

Certifique-se de ter o Python 3.x instalado. Voc√™ pode verificar a vers√£o do Python com o seguinte comando:

```bash
python3 --version
```

Se o Python n√£o estiver instalado, voc√™ pode baix√°-lo em python.org.

#### 1.2. Crie um ambiente virtual (opcional, mas recomendado)

Para isolar as depend√™ncias do projeto, √© recomendado criar um ambiente virtual. Execute os seguintes comandos:

```bash
python3 -m venv meu_ambiente  # Cria um ambiente virtual chamado "meu_ambiente"
source meu_ambiente/bin/activate  # Ativa o ambiente virtual (Linux/Mac)
# ou
meu_ambiente\Scripts\activate  # Ativa o ambiente virtual (Windows)
```

#### 1.3. Instale as depend√™ncias

O projeto depende de duas bibliotecas principais que trabalham em conjunto:

- **llama-cpp-python**: Uma interface Python para o llama.cpp, permitindo que voc√™ execute scripts Python para interagir com modelos de linguagem.
- **llama.cpp**: Uma implementa√ß√£o eficiente em C++ para executar modelos de linguagem, otimizada para desempenho em CPUs e GPUs.

Para garantir que o projeto funcione corretamente e com o m√°ximo de desempenho, voc√™ precisar√° instalar ambos.

##### 1.3.1. Instale o llama-cpp-python

O llama-cpp-python √© a biblioteca que permite integrar o llama.cpp com scripts Python. Instale-o via pip:

```bash
pip install llama-cpp-python
```

No entanto, para obter o melhor desempenho, √© altamente recomend√°vel compilar o llama.cpp manualmente e configurar o llama-cpp-python para us√°-lo.

##### 1.3.2. Compile o llama.cpp manualmente (recomendado)

O llama.cpp √© o n√∫cleo que executa os modelos de linguagem de forma eficiente. Compil√°-lo manualmente permite otimiza√ß√µes espec√≠ficas para o seu hardware, como suporte a CUDA para GPUs ou AVX2 para CPUs modernas.

Siga os passos abaixo para compilar o llama.cpp:

1. Clone o reposit√≥rio do llama.cpp:

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

2. Compile o llama.cpp:

   - Se voc√™ tiver uma GPU compat√≠vel com CUDA, compile com suporte a CUDA para acelerar o processamento:
   
   ```bash
   make LLAMA_CUBLAS=1
   ```

   - Para CPUs modernas com suporte a AVX2, compile com otimiza√ß√µes:
 
   ```bash
   make LLAMA_AVX2=1
   ```

   - Se voc√™ n√£o tiver certeza do hardware, compile com suporte b√°sico:
 
   ```bash
   make
   ```

3. Instale o llama-cpp-python com suporte √† compila√ß√£o manual:
   Ap√≥s compilar o llama.cpp, reinstale o llama-cpp-python apontando para o diret√≥rio compilado:

```bash
pip install llama-cpp-python --no-cache-dir --force-reinstall --upgrade --no-binary :all:
```

Isso garantir√° que o llama-cpp-python use a vers√£o compilada manualmente do llama.cpp, proporcionando o melhor desempenho poss√≠vel.

##### 1.3.3. Instale o tiktoken

O tiktoken √© usado para tokeniza√ß√£o avan√ßada e pode ser instalado diretamente via pip:

```bash
pip install tiktoken
```

#### 1.4. Verifique a instala√ß√£o

Ap√≥s instalar as depend√™ncias, verifique se tudo est√° funcionando corretamente:

```bash
python3 -c "import llama_cpp; import tiktoken; print('Depend√™ncias instaladas com sucesso!')"
```

Se n√£o houver erros, as depend√™ncias foram instaladas corretamente.

#### 1.5. Baixe o modelo de linguagem

O projeto utiliza um modelo de linguagem no formato GGUF. Certifique-se de que o caminho do modelo est√° configurado corretamente no arquivo config_default.json. Por exemplo:

```json
"modelo": {
    "path": "/caminho/para/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf",
    "n_ctx": 4096,
    "n_threads": 8
}
```

Se voc√™ ainda n√£o tem o modelo, pode baix√°-lo de fontes confi√°veis, como o Hugging Face. Certifique-se de escolher um modelo no formato GGUF, que √© otimizado para uso com o llama.cpp.

#### 1.6. Como llama-cpp-python e llama.cpp trabalham juntos

- **llama-cpp-python**: Fornece uma interface Python para interagir com o llama.cpp. Ele permite que voc√™ carregue modelos, gere textos e gerencie o processo de an√°lise diretamente em scripts Python.

- **llama.cpp**: √â o n√∫cleo que executa os modelos de linguagem de forma eficiente. Ele √© respons√°vel por toda a computa√ß√£o pesada, como infer√™ncia de modelos e gera√ß√£o de tokens.

Ao compilar o llama.cpp manualmente, voc√™ garante que ele esteja otimizado para o seu hardware, enquanto o llama-cpp-python facilita a integra√ß√£o com o c√≥digo Python do projeto.

#### 1.7. Ganhos de Performance com Compila√ß√£o Manual

Compilar o llama.cpp manualmente traz v√°rios benef√≠cios:

- **Acelera√ß√£o em GPUs**: Se voc√™ tiver uma GPU compat√≠vel com CUDA, a compila√ß√£o com suporte a CUDA pode acelerar significativamente o processamento.

- **Otimiza√ß√£o para CPUs**: Compilar com suporte a AVX2 ou outras instru√ß√µes modernas de CPU pode melhorar o desempenho em at√© 2x ou mais.

- **Controle total**: A compila√ß√£o manual permite ajustar o c√≥digo para o seu hardware espec√≠fico, maximizando a efici√™ncia.

Se voc√™ n√£o compilar manualmente, o projeto ainda funcionar√°, mas pode n√£o atingir o desempenho m√°ximo.

### 2. Configura√ß√£o

Edite o arquivo config_default.json para ajustar os par√¢metros do modelo e da an√°lise. 

### 3. Execu√ß√£o

Execute o script com a pergunta filos√≥fica que deseja analisar:

```bash
python decartes.py --pergunta "Podem as m√°quinas pensar?"
```

Caso deixe em default, a pergunta que estiver dentro do c√≥digo ser√° executada:

```python
parser.add_argument("--pergunta", type=str, default="Podem as m√°quinas pensar?")
```

Os resultados ser√£o salvos no diret√≥rio resultados/, com arquivos JSON e TXT contendo a an√°lise inicial, refinamentos e logs.

## Exemplo de Sa√≠da

### Logs (analise_20250308_010715_log.txt)

```
[2025-03-08 01:07:15] === IN√çCIO DA AN√ÅLISE ===
[2025-03-08 01:07:15] Pergunta: Podem as m√°quinas pensar?
[2025-03-08 01:07:15] Par√¢metros: max_tokens=1024, temperatura=0.75, max_refinamentos=5
...
```

### An√°lise Final (analise_20250308_010715_final.json)

O arquivo analise_20250308_010715_final.json √© o resultado final da an√°lise cartesiana realizada pelo projeto. Ele cont√©m todas as informa√ß√µes geradas durante o processo de an√°lise, incluindo a pergunta original, as subquest√µes geradas, as respostas iniciais e os refinamentos realizados.

#### Estrutura do Arquivo JSON

O arquivo √© organizado da seguinte forma:

```json
{
    "pergunta": "Podem as m√°quinas pensar?",
    "analise": [
        "Etapa: 1. 0.\nResposta: a. A m√°quina n√£o tem alma: V  [L√≥gico]\n             b. A m√°quina n√£o tem consci√™ncia: V\n             ..."
    ],
    "refinamentos": [
        "Considerar as contradi√ß√µes detectadas, pressupostos n√£o questionados e lacunas l√≥gicas na an√°lise original..."
    ],
    "timestamp": "20250308_010715"
}
```

#### Campos Explicados

- **pergunta**:
  - Cont√©m a pergunta filos√≥fica que foi analisada. No exemplo, a pergunta √©: "Podem as m√°quinas pensar?".

- **analise**:
  - √â uma lista de respostas geradas para cada subquest√£o elementar. Cada item na lista representa uma etapa da an√°lise, onde a subquest√£o √© decomposta e respondida.
  - Exemplo:
    ```json
    "analise": [
        "Etapa: 1. 0.\nResposta: a. A m√°quina n√£o tem alma: V  [L√≥gico]\n             b. A m√°quina n√£o tem consci√™ncia: V\n             ..."
    ]
    ```
  - Aqui, a subquest√£o "1. 0." foi respondida com uma an√°lise l√≥gica, indicando que a m√°quina n√£o tem alma nem consci√™ncia.

- **refinamentos**:
  - Cont√©m os ciclos de refinamento aplicados √† an√°lise inicial. Cada refinamento √© uma tentativa de melhorar a an√°lise, considerando contradi√ß√µes, pressupostos n√£o questionados e lacunas l√≥gicas.
  - Exemplo:
    ```json
    "refinamentos": [
        "Considerar as contradi√ß√µes detectadas, pressupostos n√£o questionados e lacunas l√≥gicas na an√°lise original..."
    ]
    ```
  - Neste caso, o refinamento sugere revisar as contradi√ß√µes e pressupostos da an√°lise inicial.

- **timestamp**:
  - Indica a data e hora em que a an√°lise foi realizada, no formato AAAAMMDD_HHMMSS. No exemplo, o timestamp "20250308_010715" significa que a an√°lise foi realizada em 8 de mar√ßo de 2025, √†s 01:07:15.

#### Como o Arquivo √© Gerado

O arquivo analise_20250308_010715_final.json √© gerado automaticamente pelo script decartes.py ap√≥s a conclus√£o da an√°lise. Ele √© salvo no diret√≥rio resultados/ com um nome √∫nico baseado no timestamp, permitindo que v√°rias an√°lises sejam armazenadas sem conflitos.

#### Exemplo Completo

Aqui est√° um exemplo completo de como o arquivo pode ser estruturado:

```json
{
    "pergunta": "Podem as m√°quinas pensar?",
    "analise": [
        "Etapa: 1. 0.\nResposta: a. A m√°quina n√£o tem alma: V  [L√≥gico]\n             b. A m√°quina n√£o tem consci√™ncia: V\n             ...",
        "Etapa: 2. 1.\nResposta: - A m√°quina pode executar c√°lculos complexos, mas n√£o possui inten√ß√£o ou vontade."
    ],
    "refinamentos": [
        "Refinamento 1: Identificada contradi√ß√£o na defini√ß√£o de 'pensar'. A m√°quina pode processar informa√ß√µes, mas n√£o possui consci√™ncia.",
        "Refinamento 2: Pressuposto n√£o questionado: 'Pensar' requer consci√™ncia. Isso pode n√£o ser verdade para todos os tipos de pensamento."
    ],
    "timestamp": "20250308_010715"
}
```

#### Como Usar o Arquivo

- **Revis√£o da An√°lise**: Voc√™ pode abrir o arquivo JSON em qualquer editor de texto ou visualizador JSON para revisar as respostas e refinamentos.

- **Integra√ß√£o com Outras Ferramentas**: O formato JSON facilita a integra√ß√£o com outras ferramentas ou scripts para an√°lise adicional ou visualiza√ß√£o.

- **Compara√ß√£o entre An√°lises**: Como cada an√°lise √© salva com um timestamp √∫nico, voc√™ pode comparar diferentes execu√ß√µes do experimento para ver como a an√°lise evoluiu ao longo do tempo.

## Melhorias Futuras

- Texto final melhor formatado com a resposta √† pergunta e um resumo do experimento, seja no come√ßo ou no final.
- Utiliza√ß√£o de um modelo mais avan√ßado de LLM (atualmente usa o modelo DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf)
- Otimiza√ß√£o do Refinamento: Implementar t√©cnicas para evitar repeti√ß√µes e garantir an√°lises mais concisas.
- Gera√ß√£o de Subquest√µes: Aprimorar a gera√ß√£o de subquest√µes para evitar redund√¢ncias.
- Integra√ß√£o com Modelos de Resumo: Utilizar t√©cnicas avan√ßadas de NLP para melhorar o resumo autom√°tico.
- Documenta√ß√£o: Expandir a documenta√ß√£o para incluir exemplos detalhados e guias de contribui√ß√£o.

## Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir issues ou enviar pull requests com melhorias, corre√ß√µes ou novas funcionalidades.

## Licen√ßa

Este projeto est√° licenciado sob a licen√ßa MIT. Consulte o arquivo LICENSE para mais detalhes.
